\documentclass[11pt,twocolumn]{article}
\usepackage{fullpage}
\usepackage{natbib}
\title{Combined Hyper-parameter Optimization\\and Ensemble Construction}
\author{James Bergstra, Dan Yamins, Nicolas Pinto, David Cox}
\begin{document}
\maketitle


\section{Introduction}

Many learning algorithms are actually algorithm families parametrized by 
``hyper-parameters,"  parameters that are fixed before the algorithm 
actually runs on a given dataset and which characterize various options for how
the algorithm will operate.

\vspace{11pt}
In this context, it is almost always possible to improve performance 
by:
\begin{enumerate}
 \item performing a high-throughput search of hyper-parameter space, and/or
 \item building an ensemble of many complementary models.
\end{enumerate}

\vspace{11pt}
Work such as
\cite{Pinto-2009}, \cite{pinto+cox:2011}, \cite{coates+lee+ng:2011},
\cite{gehler+nowozin:2009}, \cite{bergstra+bardenet+bengio+kegl:2011}
demonstrates that the combination of these two techniques can set new
records on competitive benchmarks, without \emph{any} innovative modeling or
learning algorithms.  In fact, the meaning of statements of the form
``approach X achieves accuracy Y on dataset Z'' has been fundamentally undermined by such
advances, since accuracy is so strongly dependent on parameter search and
ensemble construction strategies.

\vspace{11pt}
These two techniques increase accuracy at the expense of 
additional computation time, and when hyper-parameter 
optimization is carried out by hand, it is often
difficult to characterize the relationship between search 
effort and optimal performance.  Recent work in hyper-parameter
optimization in learning algorithms suggests how automated approaches
can help.

\vspace{11pt}
\cite{bergstra+bengio:2012} shows that random search is much more
efficient than grid search, at least for hyper-parameter optimization in
neural networks and deep belief networks
\cite{hinton+osindero+teh:2006}.
\cite{bergstra+bardenet+bengio+kegl:2011} shows that algorithms based on
sequential model-based optimization (Bayesian optimization,
see e.g. \cite{hutter:2009})
can be both practical and highly profitable.

\vspace{11pt}
However, despite these advances, hyperparameter optimization techniques have not
yet been brought to bear on ensemble construction.  Works such as
\cite{gehler+nowozin:2009}, \cite{coates+lee+ng:2011},
and \cite{pinto+cox:2011} highlight the importance of learning ensembles, so there is an incentive
to unify the approaches.

\vspace{11pt}
This work is based on the insight that, for models consisting of feature extraction followed by classification,
an ensemble construction meta-algorithm can be seen simply as an
efficient sequential strategy for hyper-parameter optimization of
models with a larger number of features.

\vspace{11pt}
More specifically, we investigate a variety of strategies for combining sequential model-based
optimization approaches to hyper-parameter optimization with boosting
approaches to ensemble construction \cite{mason+baxter+bartlett+fraen:1999,
friedman:1999}.
The objective is a black-box algorithm that is able to match or exceed the
results of published results which used additional domain knowledge or
heuristic manual search.

\section{Contribution}

Describe the algorithm.

List the six experiments that we plan to perform.

Illustrate the approach by its performance on the Boston housing regression problem.

\small
\bibliographystyle{unsrt}
\bibliography{local}

\end{document}

