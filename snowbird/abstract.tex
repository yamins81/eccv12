\documentclass[11pt,twocolumn]{article}
\usepackage{fullpage}
\usepackage{natbib}
\title{Combined Hyper-parameter Optimization\\and Ensemble Construction}
\author{James Bergstra, Dan Yamins, David Cox}
\begin{document}
\maketitle


\section{Introduction}

It is almost always possible to improve the accuracy of a learning algorithm
by one or both of the following two techniques:
\begin{enumerate}
 \item search the space of hyper-parameters more exhaustively,
 \item average together multiple models in an ensemble.
\end{enumerate}

\vspace{11pt}
These two techniques can be used to increase accuracy
by investing in additional computational time.
Work such as \citet{plos_ht}, \citet{fg11}, \citet{coates+lee+ng:2010},
\citet{gehler}, \citet{bergstra+bardenet+bengio+kegl:2011}
demonstrates that the combination of these two techniques can set new records on
benchmark problems in competitive research fields,
without any innovative modeling or learning algorithms.
The validity and utility of statements of the form
``approach X scores accuracy Y on dataset Z,''
are undermined by such advances; accuracy is
affected significantly by the hyper-parameter search strategy and the
number of features in the full ensemble, and that dependence must
be reflected in the characterization of the performance of a model family
on a particular dataset.

\vspace{11pt}
When hyper-parameter optimization is carried out by hand, it is difficult
to characterize the relationship between search effort and the accuracy of the
resulting model. However, recent work in hyper-parameter optimization in
learning algorithms suggests that hyper-parameter optimization should not be done by hand.
\citet{bergstra+bengio:2012} shows that random search is much more efficient
than grid search,
at least for hyper-parameter optimization in neural networks and deep belief networks \citep{hinton+osinero+teh:2006}.
\citet{bergstra+bardenet+bengio+kegl:2011} shows that algorithms based on
sequential model-based optimization (Bayesian optimization,
see \citep{hutter:2006}, \citet{brochu:2008})
can be both practical and profitable.

\vspace{11pt}
Despite these advances in hyper-parameter optimization, there remains the 
importance of model averaging.
However hyper-parameter optimization is not an efficient strategy to search
the space of ensembles.
At the same time, works such as \citet{gehler}, \citet{coates+lee+ng:2010}, and \citet{fg11}
highlight the importance of learning ensembles in order to achieve the best performance
on a given dataset.

\vspace{11pt}
The objection that ensemble learning is orthogonal to hyper-parameter
optimization is difficult to sustain.
If a model has the form of a feature extractor and a classifier,
and if the number of features is a hyper-parameter that's up for optimization,
then ensemble construction can be seen simply an efficient way
of searching in the space of models that have large numbers of features.
In this light, ensemble learning is a sequential strategy for hyper-parameter
optimization.

\vspace{11pt}
This work investigates strategies for combining sequential model-based optimization
approaches to hyper-parameter optimization
with boosting approaches to ensemble construction \citep{mason,friedman}.
The objective is a black-box algorithm that is able to match or exceed the
results of published results which used additional domain knowledge or heuristic
manual search.

\section{Contribution}

Describe the algorithm.

List the six experiments that we plan to perform.

Illustrate the approach by its performance on the Boston housing regression problem.

\end{document}

