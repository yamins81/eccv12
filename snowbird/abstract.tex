\documentclass[11pt,twocolumn]{article}
\usepackage{fullpage}
\usepackage{natbib}
\title{Combined Hyper-parameter Optimization\\and Ensemble Construction}
\author{James Bergstra, Dan Yamins, Nicolas Pinto, David Cox}
\begin{document}
\maketitle


\section{Introduction}

Many learning algorithms are actually algorithm families parametrized by 
``hyper-parameters,"  parameters that are fixed before the algorithm 
actually runs on a given dataset and which characterize various options for how
the algorithm will operate.

\vspace{11pt}
In this context, it is almost always possible to improve performance 
by:
\begin{enumerate}
 \item performing a high-throughput search of hyper-parameter space, and/or
 \item building an ensemble of many complementary models.
\end{enumerate}

\vspace{11pt}
Work such as
\citet{plos_ht}, \citet{fg11}, \citet{coates+lee+ng:2010},
\citet{gehler}, \citet{bergstra+bardenet+bengio+kegl:2011}
demonstrates that the combination of these two techniques can set new
records on competitive benchmarks, without \emph{any} innovative modeling or
learning algorithms.  In fact, the meaning of statements of the form
``approach X achieves accuracy Y on dataset Z'' has been fundamentally undermined by such
advances, since accuracy is so strongly dependent on parameter search and
ensemble construction strategies.

\vspace{11pt}
These two techniques increase accuracy at the expense of 
additional computation time, and when hyper-parameter 
optimization is carried out by hand, it is often
difficult to characterize the relationship between search 
effort and optimal performance.  Recent work in hyper-parameter
optimization in learning algorithms suggests how automated approaches
can help.

\vspace{11pt}
\citet{bergstra+bengio:2012} shows that random search is much more
efficient than grid search, at least for hyper-parameter optimization in
neural networks and deep belief networks
\citep{hinton+osinero+teh:2006}.
\citet{bergstra+bardenet+bengio+kegl:2011} shows that algorithms based on
sequential model-based optimization (Bayesian optimization,
see e.g. \citep{hutter:2006}, \citet{brochu:2008})
can be both practical and highly profitable.

\vspace{11pt}
However, despite these advances, hyperparameter optimization techniques have not 
yet been brought to bear on ensemble construction.  Works such as \citet{gehler}, \citet{coates+lee+ng:2010},
and \citet{fg11} highlight the importance of learning ensembles, so there is an incentive
to unify the approaches.  

\vspace{11pt}
This work is based on the insight that, for models consisting of feature extraction followed by classification, 
an ensemble construction meta-algorithm can be seen simply as an
efficient sequential strategy for hyper-parameter optimization of 
models with a larger number of features.

\vspace{11pt}
More specifically, we investigate a variety of strategies for combining sequential model-based
optimization approaches to hyper-parameter optimization with boosting
approaches to ensemble construction \citep{mason,friedman}.  
The objective is a black-box algorithm that is able to match or exceed the
results of published results which used additional domain knowledge or
heuristic manual search.

\section{Contribution}

This work compares several algorithms for combining high-throughput hyperparameter searches with various ensemble construction techniques.

We begin with comparisons of four increasingly sophisticated approaches, using random hyperparameter choice: 
\begin{enumerate}
\item ``Basic random selection":  for integers $A$ and $N$, randomly sample $N$ sized-$A$ ensembles of hyper-parameter settings; compute features and train classifiers for each independently; and then take the unweighted additive combination within each ensemble.  This is a basic control against which all the following are judged. 
\item ``Standard high-throughput mixing":  for integers $A$ and $N$, randomly sample $A \times N$ hyper-parameter settings; compute features and train classifiers for each independently; and then take the unweighted additive combination of the top-$A$ best performing models.  This algorithm is the most common approach for ensemble construction in the hyper-parameter context. 
\item ``Traditional static boosting":  again take $A \times N$ independently trained random hyperparameter samples; but then sequentially choose an ensemble of $A$ models from this set using Adaboost, a standard boosting algorithm, to determine the ensemble composition and weighting.   This algorithm attempts to combine models that have complementary error patterns.  
\item ``High-throughput boosting":  in each of $A$ sequential rounds, choose $N$ randomly sampled hyper-parameters, and compute their features; train cross-validated classifiers within each round using the decisions boundary from the ensemble chosen up to previous round to bias the classifier training margins; and at the end of each round add to the ensemble the hyper-parameter setting that achieved the best cross-validated performance.   This algorithm engages the boosting idea more completely in the optimization process, along a spectrum between highly parallelizable ($A \ll N$) and highly sequential ($A \gg N$) versions.
\end{enumerate}

We will then investigate how optimization in hyperparameter space interacts with these appraoches by:
\begin{enumerate}
\item Running rersions of the above in which random hyper-parameter sampling is replaced with sequential hyperparameter optimization, using an adapted Graphical Parzen model algorithm for optimization.   
\item Comparing to a control consisting of $A$ independent optimized runs each of length $N$.
\end{enumerate}

In comparing these algorithms, we investigate the trade-offs between ensemble performance, runtime (both of the model selection process and the final ensemble), feature vector size, and parallelizability, using two datasets:  a comparatively small categorization problem from the UC Irvine Handwritten Digits dataset, and a larger face-verification problem from the Labeled Faces in the Wild dataset. 


\end{document}

