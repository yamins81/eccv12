Many computer vision algorithms are actually whole algorithm families
characterized by various parameters (both discrete and continuous) that govern
how the algorithm will operate.  Recent work has demonstrated that for many
standard approaches to image labeling tasks, choosing the right parameters can
matter as much to final performance as any given conceptual additions to the
model. In fact, random search on fairly simple model families have set new
records on competitive benchmarks. 

However, hyper-parameter optimization is difficult to carry out by hand,
while massive grid search is easy but inefficient.  In this work, we propose an
meta-model for enabling automated hyperparameter optimization, and
demonstrate its utility in a range of computer vision tasks.

The basis of our approach is the idea of a "Bandit," a program that accepts
parameter values and emits the performance value to be optimized. However, in
addition to simply computing performance, a bandit is required to explicitly
expose the underlying abstract expression graph of how the final performance
value is computed from the parameters to be optimized over. This allows
optimization algorithms to inspect the structure of the bandit and
automatically determine which optimization techniques to use for which variable
and how to integrate results across parameters. This in turn enables bandits
expressing different algorithms for the same task to be composed, so that
individual components of each algorithm can be easily "mixed and matched"
without having to change optimization code.

We implement this idea apply it to compare and integrate two recent model
families that have been demonstrated on to yield state-of-the-art results on
face verification and object recognition, respectively. Characterizing each
algorithm family as a bandit, we show that use of a Tree Parzen Estimator
optimizer yields high-performing parameters significantly more efficiently than
random search in both cases.  Morever, synthesizing between the two, we find
several points in the meta-model search space that outperform either of the
individual approaches alone.  More broadly, our results show that the
formalization of a meta-model represents an exciting direction for future work.

================================================================================
XXXX figure how these are to be included

[1] Pinto & Cox (FG11)
[2] Coates & Ng (ICML11)
[3] Bergstra & Bengio (JMLR)
[4] Bergstra, Bardenet, Bengio, Kegl (NIPS11)
[5] Krizhevsky (for CIFAR10)
[6] ... Learned-Miller (for LFW)

