Many computer vision algorithms are actually whole algorithm families
characterized by various parameters (both discrete and continuous) that govern
how the algorithm will operate.  Recent work has demonstrated that for many
standard approaches to image labeling tasks, choosing the right parameters can
matter as much to final performance as any given conceptual additions to the
model. In fact, random search on fairly simple model families have set new
records on competitive benchmarks. 

However, hyper-parameter optimization is difficult to carry out by hand,
while massive grid search is easy but inefficient.  In this work, we propose an
meta-model for enabling automated hyperparameter optimization, and
demonstrate its utility in a range of computer vision tasks.

The basis of our approach is to explicitly expose the underlying expression
graph of how the final performance metric to be optimized is computed from the
parameters to be optimized over. This allows generic optimization algorithms to
automatically determine which optimization techniques to use for which
parameter and how to integrate results across parameters. This in turn enables
metrics expressing different algorithms for the same task to be composed, so
that individual components of each algorithm can be easily "mixed and matched"
without having to change optimization code.

We implement this idea apply it to compare and integrate two recent model
families that have been demonstrated on to yield state-of-the-art results on
face verification and object recognition, respectively.  We show that use of a
Tree Parzen Estimator (TPE) optimizer yields high-performing parameters
significantly more efficiently than random search in both cases.  Morever,
synthesizing between the two approaches, we find several points in the
meta-model search space that outperform either of the individual approaches
alone.  More broadly, our results show that the formalization of a meta-model
represents an exciting direction for future work.

================================================================================
XXXX figure how these are to be included

[1] Pinto & Cox (FG11)
[2] Coates & Ng (ICML11)
[3] Bergstra & Bengio (JMLR)
[4] Bergstra, Bardenet, Bengio, Kegl (NIPS11)
[5] Krizhevsky (for CIFAR10)
[6] ... Learned-Miller (for LFW)

